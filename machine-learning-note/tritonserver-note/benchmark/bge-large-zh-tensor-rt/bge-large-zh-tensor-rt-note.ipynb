{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Models\n",
    "\n",
    "Download [BAAI/bge-large-zh-v1.5](https://www.modelscope.cn/models/BAAI/bge-large-zh-v1.5) to `/workspace/model-store`.\n",
    "\n",
    "```bash\n",
    "# tree -h bge-large-zh-v1.5/\n",
    "[4.0K]  bge-large-zh-v1.5/\n",
    "├── [4.0K]  1_Pooling\n",
    "│   └── [ 191]  config.json\n",
    "├── [1000]  config.json\n",
    "├── [ 124]  config_sentence_transformers.json\n",
    "├── [  47]  configuration.json\n",
    "├── [1.2G]  model.safetensors\n",
    "├── [ 349]  modules.json\n",
    "├── [1.2G]  pytorch_model.bin\n",
    "├── [ 27K]  README.md\n",
    "├── [  52]  sentence_bert_config.json\n",
    "├── [ 125]  special_tokens_map.json\n",
    "├── [ 394]  tokenizer_config.json\n",
    "├── [429K]  tokenizer.json\n",
    "└── [107K]  vocab.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Conversion\n",
    "\n",
    "```bash\n",
    "# ll /workspace/model-store/\n",
    "drwxrwxr-x  4 cherry cherry 4.0K Jan  4 22:10 bge-large-zh-v1.5\n",
    "-rw-r--r--  1 root   root   3.3M Apr  2 00:29 conversion_bs16_dy.txt\n",
    "-rw-rw-r--  1 cherry cherry 4.3K Apr 17 22:22 onnx_exporter.py\n",
    "-rwxrwxr-x  1 cherry cherry  800 Apr 17 22:27 generate_models.sh\n",
    "-rw-r--r--  1 root   root   1.3G Apr 17 22:27 model.onnx\n",
    "-rw-r--r--  1 root   root   627M Apr 17 22:28 model.plan\n",
    "-rw-rw-r--  1 cherry cherry 3.4M Apr 17 22:28 bge-large-zh-tensor-rt-note.ipynb\n",
    "```\n",
    "\n",
    "TensorRT conversion logs:\n",
    "\n",
    "```\n",
    "[04/17/2025-14:28:24] [I] === Trace details ===\n",
    "[04/17/2025-14:28:24] [I] Trace averages of 10 runs:\n",
    "[04/17/2025-14:28:24] [I] Average on 10 runs - GPU latency: 9.21897 ms - Host latency: 9.24823 ms (enqueue 0.875812 ms)\n",
    "[04/17/2025-14:28:24] [I] Average on 10 runs - GPU latency: 9.23761 ms - Host latency: 9.24941 ms (enqueue 0.615643 ms)\n",
    "[04/17/2025-14:28:24] [I] Average on 10 runs - GPU latency: 9.19071 ms - Host latency: 9.20902 ms (enqueue 0.72785 ms)\n",
    "...\n",
    "[04/17/2025-14:28:24] [I] \n",
    "[04/17/2025-14:28:24] [I] === Performance summary ===\n",
    "[04/17/2025-14:28:24] [I] Throughput: 110.001 qps\n",
    "[04/17/2025-14:28:24] [I] Latency: min = 7.86725 ms, max = 10.1083 ms, mean = 9.07781 ms, median = 9.20178 ms, percentile(90%) = 9.74634 ms, percentile(95%) = 9.8595 ms, percentile(99%) = 10.0066 ms\n",
    "[04/17/2025-14:28:24] [I] Enqueue Time: min = 0.400635 ms, max = 1.18506 ms, mean = 0.544774 ms, median = 0.494507 ms, percentile(90%) = 0.699707 ms, percentile(95%) = 0.803955 ms, percentile(99%) = 1.12378 ms\n",
    "[04/17/2025-14:28:24] [I] H2D Latency: min = 0.00537109 ms, max = 0.454285 ms, mean = 0.0131696 ms, median = 0.00854492 ms, percentile(90%) = 0.0270386 ms, percentile(95%) = 0.0299683 ms, percentile(99%) = 0.0383301 ms\n",
    "[04/17/2025-14:28:24] [I] GPU Compute Time: min = 7.85406 ms, max = 10.0188 ms, mean = 9.05992 ms, median = 9.18945 ms, percentile(90%) = 9.72595 ms, percentile(95%) = 9.82324 ms, percentile(99%) = 9.94922 ms\n",
    "[04/17/2025-14:28:24] [I] D2H Latency: min = 0.00341797 ms, max = 0.00720215 ms, mean = 0.00471602 ms, median = 0.00463867 ms, percentile(90%) = 0.00543213 ms, percentile(95%) = 0.00564575 ms, percentile(99%) = 0.00585938 ms\n",
    "[04/17/2025-14:28:24] [I] Total Host Walltime: 3.02725 s\n",
    "[04/17/2025-14:28:24] [I] Total GPU Compute Time: 3.01695 s\n",
    "[04/17/2025-14:28:24] [W] * GPU compute time is unstable, with coefficient of variance = 5.89673%.\n",
    "[04/17/2025-14:28:24] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
    "[04/17/2025-14:28:24] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
    "[04/17/2025-14:28:24] [V] \n",
    "[04/17/2025-14:28:24] [V] === Explanations of the performance metrics ===\n",
    "[04/17/2025-14:28:24] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.\n",
    "[04/17/2025-14:28:24] [V] GPU Compute Time: the GPU latency to execute the kernels for a query.\n",
    "[04/17/2025-14:28:24] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
    "[04/17/2025-14:28:24] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.\n",
    "[04/17/2025-14:28:24] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.\n",
    "[04/17/2025-14:28:24] [V] H2D Latency: the latency for host-to-device data transfers for input tensors of a single query.\n",
    "[04/17/2025-14:28:24] [V] D2H Latency: the latency for device-to-host data transfers for output tensors of a single query.\n",
    "[04/17/2025-14:28:24] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.\n",
    "[04/17/2025-14:28:24] [I] \n",
    "&&&& PASSED TensorRT.trtexec [TensorRT v100700] [b23] # trtexec --onnx=model.onnx --saveEngine=model.plan --fp16 --verbose --minShapes=input_ids:1x1,attention_mask:1x1 --optShapes=input_ids:4x256,attention_mask:4x256 --maxShapes=input_ids:32x512,attention_mask:32x512\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run --gpus=all --rm -it -v /workspace:/workspace nvcr.io/nvidia/pytorch:24.12-py3 /bin/bash /workspace/model-store/generate_models.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TritonServer\n",
    "\n",
    "we serve bge-large-zh with model ensemble in this demo, and we prepare model hierarchy with following:\n",
    "\n",
    "```bash\n",
    "# tree bge_large_zh_ensemble\n",
    "bge_large_zh_ensemble\n",
    "├── 1\n",
    "└── config.pbtxt\n",
    "\n",
    "# tree python_bge_large_zh_tokenizer\n",
    "python_bge_large_zh_tokenizer\n",
    "├── 1 # move bge-large-zh tokenizer file here\n",
    "│   ├── 1_Pooling\n",
    "│   │   └── config.json\n",
    "│   ├── config.json\n",
    "│   ├── config_sentence_transformers.json\n",
    "│   ├── configuration.json\n",
    "│   ├── model.py\n",
    "│   ├── modules.json\n",
    "│   ├── README.md\n",
    "│   ├── sentence_bert_config.json\n",
    "│   ├── special_tokens_map.json\n",
    "│   ├── tokenizer_config.json\n",
    "│   ├── tokenizer.json\n",
    "│   └── vocab.txt\n",
    "└── config.pbtxt\n",
    "\n",
    "# tree bge_large_zh_trt/\n",
    "bge_large_zh_trt/\n",
    "├── 1 # move model.plan here\n",
    "│   ├── model.plan\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "then start tritonserver with following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need install transformers in the image\n",
    "!docker run --gpus=all --network=host --rm -it -v /workspace/:/workspace nvcr.io/nvidia/tritonserver:24.12-py3 tritonserver --model-control-mode=explicit --load-model=bge_large_zh_ensemble --model-repository=/workspace/model-store/bge-large-zh-tensor-rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "server load:\n",
    "\n",
    "```\n",
    "# nvidia-smi\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:06:00.0  On |                  N/A |\n",
    "| 71%   79C    P0            344W /  350W |    2440MiB /  24576MiB |     99%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient.utils import np_to_triton_dtype, triton_to_np_dtype\n",
    "from tritonclient.utils import InferenceServerException\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "os.chdir(\"/workspace/model-store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "server_addr = \"localhost:8001\"\n",
    "triton_client = grpcclient.InferenceServerClient(server_addr)\n",
    "\n",
    "demo = [\"hello world\", \"nice to meet you\"]\n",
    "in0 = np.array([demo], dtype=np.object_).reshape(-1, 1)\n",
    "input_tensors = [\n",
    "    grpcclient.InferInput(\"ensemble_input\", in0.shape, np_to_triton_dtype(in0.dtype)).set_data_from_numpy(in0),\n",
    "]\n",
    "\n",
    "\n",
    "model_name = \"bge_large_zh_ensemble\"\n",
    "infer_rsp = triton_client.infer(model_name, inputs=input_tensors)\n",
    "\n",
    "output0 = infer_rsp.as_numpy(\"ensemble_dense_vecs\")\n",
    "print(type(output0), output0.shape)\n",
    "output1 = infer_rsp.as_numpy(\"ensemble_dense_token_num\")\n",
    "print(type(output1), output1.shape, output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveat\n",
    "\n",
    "- `tritonserver` stucked somehow when I profile its performance, and `nvidia-smi` shows GPU Util is 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
